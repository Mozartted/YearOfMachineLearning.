{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic classification is a method of classification pretty similar to linear regression algorithmm the difference is in what they being used for. Linear regression algorithms are used for prediction, while Logistic regression is for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression algorithm also uses a linear equation with independent predictors to predict a value. The predicted value can be anywhere between negative infinity to positive infinity. We need the output of the algorithm to be class variable, i.e 0-no, 1-yes. Therefore, we are squashing the output of the linear equation into a range of [0,1]. To squash the predicted value between 0 and 1, we use the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regression algorithm is binary in nature, so we are going to deal with just two datasets. Therefore we would ignore the Iris-virginica flower. and focus on the two available to us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./dataset/iris/iris.csv') ## Load data\n",
    "df = df.drop(['Id'],axis=1)\n",
    "rows = list(range(100,150))\n",
    "df = df.drop(df.index[rows])  ## Drop the rows with target values Iris-virginica\n",
    "Y = []\n",
    "target = df['Species']\n",
    "for val in target:\n",
    "    if(val == 'Iris-setosa'):\n",
    "        Y.append(0)\n",
    "    else:\n",
    "        Y.append(1)\n",
    "df = df.drop(['Species'],axis=1)\n",
    "X = df.values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare and train our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/machinelearning/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X, Y = shuffle(X,Y)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.9)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "x_1 = x_train[:,0]\n",
    "x_2 = x_train[:,1]\n",
    "x_3 = x_train[:,2]\n",
    "x_4 = x_train[:,3]\n",
    "\n",
    "x_1 = np.array(x_1)\n",
    "x_2 = np.array(x_2)\n",
    "x_3 = np.array(x_3)\n",
    "x_4 = np.array(x_4)\n",
    "\n",
    "x_1 = x_1.reshape(90,1)\n",
    "x_2 = x_2.reshape(90,1)\n",
    "x_3 = x_3.reshape(90,1)\n",
    "x_4 = x_4.reshape(90,1)\n",
    "\n",
    "y_train = y_train.reshape(90,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 4 major predictors in our dataset, we extract each of them into a individual vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Logistic Regression \n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "m = 90\n",
    "alpha = 0.0001\n",
    "\n",
    "theta_0 = np.zeros((m,1))\n",
    "theta_1 = np.zeros((m,1))\n",
    "theta_2 = np.zeros((m,1))\n",
    "theta_3 = np.zeros((m,1))\n",
    "theta_4 = np.zeros((m,1))\n",
    "\n",
    "\n",
    "epochs = 0\n",
    "cost_func = []\n",
    "while(epochs < 10000):\n",
    "    y = theta_0 + theta_1 * x_1 + theta_2 * x_2 + theta_3 * x_3 + theta_4 * x_4\n",
    "    y = sigmoid(y)\n",
    "    \n",
    "    cost = (- np.dot(np.transpose(y_train),np.log(y)) - np.dot(np.transpose(1-y_train),np.log(1-y)))/m\n",
    "    \n",
    "    theta_0_grad = np.dot(np.ones((1,m)),y-y_train)/m\n",
    "    theta_1_grad = np.dot(np.transpose(x_1),y-y_train)/m\n",
    "    theta_2_grad = np.dot(np.transpose(x_2),y-y_train)/m\n",
    "    theta_3_grad = np.dot(np.transpose(x_3),y-y_train)/m\n",
    "    theta_4_grad = np.dot(np.transpose(x_4),y-y_train)/m\n",
    "    \n",
    "    theta_0 = theta_0 - alpha * theta_0_grad\n",
    "    theta_1 = theta_1 - alpha * theta_1_grad\n",
    "    theta_2 = theta_2 - alpha * theta_2_grad\n",
    "    theta_3 = theta_3 - alpha * theta_3_grad\n",
    "    theta_4 = theta_4 - alpha * theta_4_grad\n",
    "    \n",
    "    cost_func.append(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the parameters(theta_0, theta_1, …) with 0. During each epoch, we calculate the values using the linear equation, squash the values within the range of 0 to 1 and then we calculate the cost. From the cost function, we calculate the gradients for each parameter and update their values by multiplying gradients with alpha. Alpha is the learning rate of the algorithm. After 10000 epochs, our algorithm would’ve converged to the minima. We can test our algorithm with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_x_1 = x_test[:,0]\n",
    "test_x_2 = x_test[:,1]\n",
    "test_x_3 = x_test[:,2]\n",
    "test_x_4 = x_test[:,3]\n",
    "\n",
    "test_x_1 = np.array(test_x_1)\n",
    "test_x_2 = np.array(test_x_2)\n",
    "test_x_3 = np.array(test_x_3)\n",
    "test_x_4 = np.array(test_x_4)\n",
    "\n",
    "test_x_1 = test_x_1.reshape(10,1)\n",
    "test_x_2 = test_x_2.reshape(10,1)\n",
    "test_x_3 = test_x_3.reshape(10,1)\n",
    "test_x_4 = test_x_4.reshape(10,1)\n",
    "\n",
    "index = list(range(10,90))\n",
    "\n",
    "theta_0 = np.delete(theta_0, index)\n",
    "theta_1 = np.delete(theta_1, index)\n",
    "theta_2 = np.delete(theta_2, index)\n",
    "theta_3 = np.delete(theta_3, index)\n",
    "theta_4 = np.delete(theta_4, index)\n",
    "\n",
    "theta_0 = theta_0.reshape(10,1)\n",
    "theta_1 = theta_1.reshape(10,1)\n",
    "theta_2 = theta_2.reshape(10,1)\n",
    "theta_3 = theta_3.reshape(10,1)\n",
    "theta_4 = theta_4.reshape(10,1)\n",
    "\n",
    "y_pred = theta_0 + theta_1 * test_x_1 + theta_2 * test_x_2 + theta_3 * test_x_3 + theta_4 * test_x_4\n",
    "y_pred = sigmoid(y_pred)\n",
    "\n",
    "new_y_pred =[]\n",
    "for val in y_pred:\n",
    "    if(val >= 0.5):\n",
    "        new_y_pred.append(1)\n",
    "    else:\n",
    "        new_y_pred.append(0)\n",
    "\n",
    "print(accuracy_score(y_test,new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the test data features similar to the training data. We also clip the values of theta_0, theta_1, theta_2, theta_3 and theta_4 from 90x1 to 10x1 as the number of testing examples is only 10. We calculate the test classes and check the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "cost_func = np.array(cost_func)\n",
    "cost_func = cost_func.reshape(10000,1)\n",
    "plt.plot(range(len(cost_func)),cost_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that was a lot of coding for a simple classifer like this, well sklearn provides us with a Logistic regression algorithm off the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machinelearning]",
   "language": "python",
   "name": "conda-env-machinelearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
